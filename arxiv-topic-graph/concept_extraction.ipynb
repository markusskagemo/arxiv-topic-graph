{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import preprocessing\n",
    "\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time series forecasting is a crucial component of many important applications, ranging from forecasting the stock markets to energy load prediction. The high-dimensionality, velocity and variety of th'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#texts = preprocessing.corpus_tokens()\n",
    "#dictionary = corpora.Dictionary(texts)\n",
    "flat_metadata = preprocessing.flat_unique()\n",
    "summaries = [paper['summary'].replace('\\n', ' ') for paper in flat_metadata]\n",
    "\n",
    "# Sample\n",
    "summaries[0][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9232/9232 [07:04<00:00, 25.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "corpus_orgs = []\n",
    "for i in tqdm(range(len(summaries))):\n",
    "    doc = nlp(summaries[i])\n",
    "    doc_orgs = []\n",
    "    for entity in doc.ents:\n",
    "        #print(entity.text, entity.label_)\n",
    "        if entity.label_ == 'ORG':\n",
    "            text = entity.text\n",
    "            #print(text)\n",
    "            doc_orgs.append(text)\n",
    "    corpus_orgs.append(doc_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pylab import draw_networkx, draw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gograph():\n",
    "    G = nx.Graph()\n",
    "    #G.add_edge('A', 'B', weight=4)\n",
    "    #nx.shortest_path(G, 'A', 'B', weight='weight')\n",
    "    sim = []\n",
    "    N = 50\n",
    "    #for i in range(1, len(summaries)):\n",
    "    for i in tqdm(range(1, N)):\n",
    "        for j in range(1, N):\n",
    "            # Determine semantic similarities\n",
    "            doc1 = nlp(summaries[i])\n",
    "            doc2 = nlp(summaries[j])\n",
    "            similarity = doc1.similarity(doc2)\n",
    "\n",
    "            G.add_edge(str(j), str(i), weight=1-similarity)\n",
    "\n",
    "    try:\n",
    "        draw_networkx(G)\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        plt.draw()\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.xlim(0.87, 1)\n",
    "    sns.distplot(sim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-wiki-gigaword-300\")  # download the model and return as object ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import gensim\n",
    "\n",
    "\n",
    "def read_corpus(metadata_path='data/arxiv_metadata.json', tokens_only=False):\n",
    "    arx = preprocessing.flat_unique(path=metadata_path)\n",
    "    for i, paper in enumerate(arx):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(paper['title'] + paper['summary'])\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(paper['title'] + paper['summary']), [i])\n",
    "    \n",
    "    \n",
    "#test = list(read_corpus(tokens_only=True))\n",
    "train_corpus = list(read_corpus(tokens_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 21s, sys: 5.46 s, total: 9min 27s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=100)\n",
    "model.build_vocab(train_corpus) # ?\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (On possible origins of trends in financial market price changes): «We investigate possible origins of trends using a deterministic threshold\n",
      "model, where we refer to long-term variabilities of price changes (price\n",
      "movements) in financial markets as trends. From the investigation we find two\n",
      "phenomena. One is that the trend of monotonic increase and decrease can be\n",
      "generated by dealers' minuscule change in mood, which corresponds to the\n",
      "possible fundamentals. The other is that the emergence of trends is all but\n",
      "inevitable in the realistic situation because of the fact that dealers cannot\n",
      "always obtain accurate information about deals, even if there is no influence\n",
      "from fundamentals and technical analyses.»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d300,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (2048, 0.20832426846027374): «We present a solution for the problems related to the application of\n",
      "multivariate Garch models to markets with a large number of stocks by\n",
      "restricting the form of the covariance matrix. It contains one component\n",
      "describing the market and a second simple component to account for the\n",
      "remaining contribution to the volatility. This allows the analytical\n",
      "calculation of the inverse covariance matrix. We compare our model with the\n",
      "results of other Garch models for the daily returns from the S&P500 market. The\n",
      "description of the covariance matrix turns out to be similar to the DCC model\n",
      "but has fewer free parameters and requires less computing time. The model also\n",
      "has the advantage that it contains the calculation of dynamic beta values. As\n",
      "applications we use the daily values of $\\beta$ coefficients available from the\n",
      "market component to confirm a transition of the market in 2006. Further we\n",
      "discuss the relationship of our model with the leverage effect.»\n",
      "\n",
      "MEDIAN (3646, 0.01480510551482439): «Even when confronted with the same data, agents often disagree on a model of\n",
      "the real-world. Here, we address the question of how interacting heterogenous\n",
      "agents, who disagree on what model the real-world follows, optimize their\n",
      "trading actions. The market has latent factors that drive prices, and agents\n",
      "account for the permanent impact they have on prices. This leads to a large\n",
      "stochastic game, where each agents' performance criteria is computed under a\n",
      "different probability measure. We analyse the mean-field game (MFG) limit of\n",
      "the stochastic game and show that the Nash equilibria is given by the solution\n",
      "to a non-standard vector-valued forward-backward stochastic differential\n",
      "equation. Under some mild assumptions, we construct the solution in terms of\n",
      "expectations of the filtered states. We prove the MFG strategy forms an\n",
      "\\epsilon-Nash equilibrium for the finite player game. Lastly, we present a\n",
      "least-squares Monte Carlo based algorithm for computing the optimal control and\n",
      "illustrate the results through simulation in market where agents disagree on\n",
      "the model.»\n",
      "\n",
      "LEAST (5014, -0.19037112593650818): «Portfolio turnpikes state that, as the investment horizon increases, optimal\n",
      "portfolios for generic utilities converge to those of isoelastic utilities.\n",
      "This paper proves three kinds of turnpikes. In a general semimartingale\n",
      "setting, the abstract turnpike states that optimal final payoffs and portfolios\n",
      "converge under their myopic probabilities. In diffusion models with several\n",
      "assets and a single state variable, the classic turnpike demonstrates that\n",
      "optimal portfolios converge under the physical probability; meanwhile the\n",
      "explicit turnpike identifies the limit of finite-horizon optimal portfolios as\n",
      "a long-run myopic portfolio defined in terms of the solution of an ergodic HJB\n",
      "equation.»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get summaries\n",
    "train_summaries = [paper['summary'] for paper in preprocessing.flat_unique()]\n",
    "train_titles = [paper['title'] for paper in preprocessing.flat_unique()]\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_summaries) - 1)\n",
    "inferred_vector = model.infer_vector(train_summaries[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(train_titles[doc_id], train_summaries[doc_id]))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], train_summaries[sims[index][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
