{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import preprocessing\n",
    "from gensim import corpora\n",
    "\n",
    "import os\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def texts_corpus(textdir='data/texts/'):\n",
    "    \"\"\"Create a preprocessed corpus for doc2vec training.\n",
    "    \n",
    "    Parameters:\n",
    "        textdir: str\n",
    "            Path of text files\n",
    "    Returns:\n",
    "        corpus: list of gensim.models.doc2vec.TaggedDocument objects\n",
    "            Tagged by arxiv ID\n",
    "    \"\"\"\n",
    "    files = os.listdir(textdir)\n",
    "    corpus = []\n",
    "    for i, file in enumerate(files):\n",
    "        if '.txt' not in file:\n",
    "            continue\n",
    "        with open(textdir + file) as f:\n",
    "            t = f.read()\n",
    "            corpus.append(\n",
    "                TaggedDocument(\n",
    "                    words=preprocessing.doc_preprocessor(t, lemmatize=False),\n",
    "                    tags=[file.strip('.txt')])\n",
    "            )\n",
    "            \n",
    "    return corpus\n",
    "            \n",
    "train_corpus = texts_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "\"\"\"model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\"\"\"\n",
    "#model.save('data/doc2vec.model_1')\n",
    "model = Doc2Vec.load('data/doc2vec_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_id_to_tag():\n",
    "    fu_dict = {\n",
    "        paper['id'].split('/')[-1]: paper \\\n",
    "        for paper in preprocessing.flat_unique()\n",
    "    }\n",
    "    id_term = {}\n",
    "    for key in fu_dict.keys():\n",
    "        id_term[key] = [\n",
    "            tag['term'].split('.')[1] for tag in fu_dict[key]['tags'] \\\n",
    "             if tag['term'].split('.')[0] == 'q-fin'\n",
    "        ][0]\n",
    "\n",
    "    return id_term\n",
    "\n",
    "\n",
    "def arxiv_id_to_title():\n",
    "    id_to_title = {\n",
    "        paper['id'].split('/')[-1]: paper['title'] \\\n",
    "        for paper in preprocessing.flat_unique()\n",
    "    }\n",
    "    return id_to_title\n",
    "\n",
    "\n",
    "id_term = arxiv_id_to_tag()\n",
    "id_to_title = arxiv_id_to_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(id_term.values())\n",
    "\"\"\"colors = {\n",
    "    'CP': 'red', 'EC': 'blue', \n",
    "    'GN': 'green', 'MF': 'grey', \n",
    "    'PM': 'black', 'PR': 'yellow',\n",
    "    'RM': 'violet', 'ST': 'orange',\n",
    "    'TR': 'pink'\n",
    "}\"\"\"\n",
    "colors = {\n",
    "    'CP': '#e6194b', 'EC': '#4363d8', \n",
    "    'GN': '#f58231', 'MF': '#ffe119', \n",
    "    'PM': '#806080', 'PR': '#3cb44b',\n",
    "    'RM': '#008080', 'ST': '#bcf60c',\n",
    "    'TR': '#911eb4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "json_graph = {\"nodes\": [], \"edges\": []}\n",
    "created_nodes = []\n",
    "edge_pairs = []\n",
    "for i in range(0, len(model.docvecs), 5):\n",
    "    ivec = model.infer_vector(doc_words=train_corpus[i].words) # > Not robust enough\n",
    "    sims = model.docvecs.most_similar([ivec], topn=len(model.docvecs))\n",
    "    \n",
    "    this_node = sims[0][0] # > Not robust enough\n",
    "    #this_node = model.docvecs.doctags[i]\n",
    "    \n",
    "    if this_node not in created_nodes:\n",
    "        # To vis.js\n",
    "        json_graph[\"nodes\"].append({\n",
    "            \"id\": this_node, \n",
    "            \"label\": id_to_title[this_node], \n",
    "            \"shape\": \"box\",\n",
    "            \"color\": colors[id_term[this_node]]\n",
    "        })\n",
    "        created_nodes.append(this_node)\n",
    "    \n",
    "    # Add most similar distance\n",
    "    for j in range(1, 10):\n",
    "        edge_pair = set([this_node, sims[j][0]])\n",
    "        if edge_pair not in edge_pairs:\n",
    "            # To vis.js\n",
    "            json_graph[\"edges\"].append({\n",
    "                \"from\": this_node, \n",
    "                \"to\": sims[j][0], \n",
    "                \"length\": (1-sims[j][1]),\n",
    "                \"color\": {\"color\": colors[id_term[sims[j][0]]]}\n",
    "            })\n",
    "            # Networkx edge append\n",
    "            G.add_edge(\n",
    "                this_node, \n",
    "                sims[j][0],        \n",
    "                attr_dict={\n",
    "                    'distance': (1-sims[j][1]), \n",
    "                    'color': colors[id_term[sims[j][0]]]}\n",
    "            )\n",
    "            edge_pairs.append(edge_pair)\n",
    "        else:\n",
    "            #print('Skipped double edge.')\n",
    "            pass\n",
    "\n",
    "        \n",
    "with open('data/graph.json', 'w') as fp:\n",
    "    json.dump(json_graph, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_delete = {'nodes': [], 'edges': []}\n",
    "for node in json_graph['nodes']:\n",
    "    connected_edges = [edge for edge in json_graph['edges'] \\\n",
    "     if node['id'] in [edge['to'], edge['from']]]\n",
    "    \n",
    "    #print(len(connected_edges))\n",
    "    \"\"\"\n",
    "    for edge in json_graph['edges']:\n",
    "        if node['id'] in [edge['to'], edge['from']]:\n",
    "            break\n",
    "    \"\"\"\n",
    "    to_delete['nodes'].append(node)\n",
    "    \n",
    "len(to_delete['nodes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
