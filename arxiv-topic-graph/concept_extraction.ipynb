{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import preprocessing\n",
    "\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import gensim\n",
    "\n",
    "\n",
    "def read_corpus(metadata_path='data/arxiv_metadata.json', tokens_only=False):\n",
    "    arx = preprocessing.flat_unique(path=metadata_path)\n",
    "    for i, paper in enumerate(arx):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(paper['title'] + paper['summary'])\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(\n",
    "                    gensim.utils.simple_preprocess(paper['title'] + paper['summary']), [i])\n",
    "    \n",
    "    \n",
    "#test = list(read_corpus(tokens_only=True))\n",
    "train_corpus = list(read_corpus(tokens_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def texts_corpus(textdir='data/texts/'):\n",
    "    files = os.listdir(textdir)\n",
    "    for file in files:\n",
    "        if '.txt' not in file:\n",
    "            continue\n",
    "        with open(textdir + file) as f:\n",
    "            t = f.read()\n",
    "            #yield [word for word in t.replace('\\n', ' ').split() if len(word) > 3 and word.isalpha()]\n",
    "            yield gensim.utils.simple_preprocess(t.replace('\\n', ' '))\n",
    "            \n",
    "train_corpus = texts_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-0d62a6a92a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m   1172\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         )\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                     logger.warning(\n\u001b[1;32m   1301\u001b[0m                         \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=100)\n",
    "model.build_vocab(train_corpus) # ?\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (Statistical Basis for Predicting Technological Progress): «Forecasting technological progress is of great interest to engineers, policy\n",
      "makers, and private investors. Several models have been proposed for predicting\n",
      "technological improvement, but how well do these models perform? An early\n",
      "hypothesis made by Theodore Wright in 1936 is that cost decreases as a power\n",
      "law of cumulative production. An alternative hypothesis is Moore's law, which\n",
      "can be generalized to say that technologies improve exponentially with time.\n",
      "Other alternatives were proposed by Goddard, Sinclair et al., and Nordhaus.\n",
      "These hypotheses have not previously been rigorously tested. Using a new\n",
      "database on the cost and production of 62 different technologies, which is the\n",
      "most expansive of its kind, we test the ability of six different postulated\n",
      "laws to predict future costs. Our approach involves hindcasting and developing\n",
      "a statistical model to rank the performance of the postulated laws. Wright's\n",
      "law produces the best forecasts, but Moore's law is not far behind. We discover\n",
      "a previously unobserved regularity that production tends to increase\n",
      "exponentially. A combination of an exponential decrease in cost and an\n",
      "exponential increase in production would make Moore's law and Wright's law\n",
      "indistinguishable, as originally pointed out by Sahal. We show for the first\n",
      "time that these regularities are observed in data to such a degree that the\n",
      "performance of these two laws is nearly tied. Our results show that\n",
      "technological progress is forecastable, with the square root of the logarithmic\n",
      "error growing linearly with the forecasting horizon at a typical rate of 2.5%\n",
      "per year. These results have implications for theories of technological change,\n",
      "and assessments of candidate technologies and policies for climate change\n",
      "mitigation.»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d300,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (6855, 0.14535479247570038): «In this paper we propose a look at the capital risk problem inspired by\n",
      "deterministic, known from classical mechanics, problem of juggling. We propose\n",
      "capital equivalents to the Newton's laws of motion and on this basis we\n",
      "determine the most secure form of credit repayment with regard to maximisation\n",
      "of profit. Then we extend the Newton's laws to models in linear spaces of\n",
      "arbitrary dimension with the help of matrix rates of return. The matrix rates\n",
      "describe the evolution of multidimensional capital and they are sensitive to\n",
      "both quantitative changes of individual elements and flows between them. This\n",
      "allows us for simultaneous analysis of evolution of complex capital in both\n",
      "continuous and discrete time models.»\n",
      "\n",
      "MEDIAN (1436, -0.03817196190357208): «In this paper we discuss a concept of dynamic memory and an application of\n",
      "fractional calculus to describe the dynamic memory. The concept of memory is\n",
      "considered from the standpoint of economic models in the framework of\n",
      "continuous time approach based on fractional calculus. We also describe some\n",
      "general restrictions that can be imposed on the structure and properties of\n",
      "dynamic memory. These restrictions include the following three principles: (a)\n",
      "the principle of fading memory; (b) the principle of memory homogeneity on time\n",
      "(the principle of non-aging memory); (c) the principle of memory reversibility\n",
      "(the principle of memory recovery). Examples of different memory functions are\n",
      "suggested by using the fractional calculus. To illustrate an application of the\n",
      "concept of dynamic memory in economics we consider a generalization of the\n",
      "Harrod-Domar model, where the power-law memory is taken into account.»\n",
      "\n",
      "LEAST (2366, -0.2238115668296814): «I sketch a program for a microeconomic theory of the main component of the\n",
      "business cycle as a recurring disequilibrium, driven by incompleteness of the\n",
      "financial market and by information asymmetries between borrowers and lenders.\n",
      "This proposal seeks to incorporate five distinct but connected processes that\n",
      "have been discussed at varying lengths in the literature: the leverage cycle,\n",
      "financial panic, debt deflation, debt overhang, and deleveraging of households.\n",
      "In the wake of the 2007-08 financial crisis, policy responses by central banks\n",
      "have addressed only financial panic and debt deflation. Debt overhang and the\n",
      "slowness of household deleveraging account for the Keynesian \"excessive saving\"\n",
      "seen in recessions, which raises questions about the suitability of the\n",
      "standard Keynesian remedies.»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "papers = preprocessing.flat_unique()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Get summaries\n",
    "train_summaries = [paper['summary'] for paper in papers]\n",
    "train_titles = [paper['title'] for paper in papers]\n",
    "train_titles = [paper['title'] for paper in papers]\n",
    "\"\"\"\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_summaries) - 1)\n",
    "inferred_vector = model.infer_vector(train_summaries[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(train_titles[doc_id], train_summaries[doc_id]))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], train_summaries[sims[index][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx = preprocessing.flat_unique()\n",
    "for i in range(len(arx)):\n",
    "    terms = [item['term'] for item in arx[i]['tags'] if 'q-fin' in item['term']][0]     \n",
    "    #print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(edges, paper):\n",
    "    authors = paper['authors']\n",
    "    for author in authors:\n",
    "        for other in authors.remove(author):\n",
    "            if other not in edges[author]:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
